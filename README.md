# Local Ollama
Simple and lightweight chat interface written in Vue for Ollama models based on the client's ollama server.

> [!WARNING]
> This project is not finished yet â€” it's just a testing demo.
> Views and components are not finalized, not optimized, and currently do not support chat history or session management.
> This is a minimal proof of concept for real-time interaction with Ollama models.

# Demo
Visit [Local Ollama](https://localollama.pages.dev/) for demo preview.
You can freely use this Git source and host it for free on [Cloudflare Pages](https://dash.cloudflare.com).

# Models not shown? (Ollama's CORS error)
Fix it using [this tutorial](https://objectgraph.com/blog/ollama-cors/)
